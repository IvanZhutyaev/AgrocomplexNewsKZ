import asyncio
import feedparser
import requests
import re
import html
from bs4 import BeautifulSoup
from config import DEEPSEEK_KEY
from database import get_sites, is_news_sent, is_news_published, mark_news_sent, add_to_queue, clear_stuck_processing, \
    get_next_from_queue, mark_queue_processed, get_queue_size
from bot import send_original_news_to_admin


# –ü–∞—Ä—Å–∏–Ω–≥ –ø–æ–ª–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ —Å—Ç–∞—Ç—å–∏
def get_full_article(url: str) -> str:
    try:
        print(f"üîç –ü–∞—Ä—Å–∏–º —Å—Ç–∞—Ç—å—é: {url}")

        # –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }

        response = requests.get(url, timeout=10, headers=headers)
        response.encoding = response.apparent_encoding
        soup = BeautifulSoup(response.text, "html.parser")

        # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ —Å–µ–ª–µ–∫—Ç–æ—Ä–æ–≤ –¥–ª—è –ø–æ–∏—Å–∫–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        selectors = [
            "article",
            "div.article",
            "div.content",
            "div.post-content",
            "div.entry-content",
            "div.story-text",
            "div.text",
            "main",
            "[role='main']",
            "div.news-text",
            "div.news-content",
            "div.news-detail",
            "div.detail-text",
            ".news__text",
            ".article__text",
            ".content__text",
            "div.news-body",
            "div.article-body"
        ]

        # –ò—â–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç –ø–æ —Å–µ–ª–µ–∫—Ç–æ—Ä–∞–º
        content = None
        for selector in selectors:
            content = soup.select_one(selector)
            if content:
                print(f"‚úÖ –ù–∞–π–¥–µ–Ω –∫–æ–Ω—Ç–µ–Ω—Ç –ø–æ —Å–µ–ª–µ–∫—Ç–æ—Ä—É: {selector}")
                break

        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –ø–æ —Å–µ–ª–µ–∫—Ç–æ—Ä–∞–º, –ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –ø–æ –∫–ª–∞—Å—Å–∞–º —Å–æ–¥–µ—Ä–∂–∞—â–∏–º "content", "text", "article"
        if not content:
            for tag in soup.find_all(['div', 'article', 'section']):
                classes = tag.get('class', [])
                if classes and any(keyword in ' '.join(classes).lower() for keyword in
                                  ['content', 'text', 'article', 'story', 'post', 'entry', 'body', 'main']):
                    content = tag
                    print(f"‚úÖ –ù–∞–π–¥–µ–Ω –∫–æ–Ω—Ç–µ–Ω—Ç –ø–æ –∫–ª–∞—Å—Å—É: {classes}")
                    break

        # –ï—Å–ª–∏ –≤—Å–µ –µ—â–µ –Ω–µ –Ω–∞—à–ª–∏, –±–µ—Ä–µ–º body
        if not content:
            content = soup.find('body')
            print("‚ö†Ô∏è –ö–æ–Ω—Ç–µ–Ω—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º body")

        # –û—á–∏—â–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç –æ—Ç –Ω–µ–Ω—É–∂–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤
        if content:
            # –£–¥–∞–ª—è–µ–º –Ω–µ–Ω—É–∂–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
            for element in content.find_all(['script', 'style', 'nav', 'header', 'footer', 'aside', 'form', 'iframe']):
                element.decompose()

            # –£–¥–∞–ª—è–µ–º —ç–ª–µ–º–µ–Ω—Ç—ã —Å –∫–ª–∞—Å—Å–∞–º–∏ —Å–æ–¥–µ—Ä–∂–∞—â–∏–º–∏ "menu", "header", "footer", "sidebar", "ad", "banner"
            for element in content.find_all(class_=re.compile(
                    r'menu|header|footer|sidebar|ad|banner|social|comment|meta|related|popular', re.I)):
                element.decompose()

            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç
            text = content.get_text(separator='\n', strip=True)

            # –û—á–∏—â–∞–µ–º —Ç–µ–∫—Å—Ç –æ—Ç –ª–∏—à–Ω–∏—Ö –ø–µ—Ä–µ–Ω–æ—Å–æ–≤ –∏ –ø—Ä–æ–±–µ–ª–æ–≤
            text = re.sub(r'\n\s*\n', '\n\n', text)
            text = re.sub(r' +', ' ', text)

            # –û–±—Ä–µ–∑–∞–µ–º –¥–æ —Ä–∞–∑—É–º–Ω–æ–π –¥–ª–∏–Ω—ã (–ø—Ä–∏–º–µ—Ä–Ω–æ 2000 —Å–∏–º–≤–æ–ª–æ–≤)
            if len(text) > 2000:
                text = text[:2000] + "..."

            return text.strip()

        return ""

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å—Ç–∞—Ç—å–∏ {url}: {e}")
        return ""


# –°–∫–∞—á–∏–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
def download_image(url: str) -> str:
    try:
        print(f"üì• –°–∫–∞—á–∏–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {url}")

        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—É—Å—Ç—ã–µ URL
        if not url or url.strip() == "":
            print("‚ùå –ü—É—Å—Ç–æ–π URL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è")
            return ""

        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }

        response = requests.get(url, timeout=15, headers=headers)
        response.raise_for_status()

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        content_type = response.headers.get('content-type', '')
        if not content_type.startswith('image/'):
            print(f"‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π content-type: {content_type}")
            return ""

        # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç
        import os
        if not os.path.exists("images"):
            os.makedirs("images")

        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏–º—è —Ñ–∞–π–ª–∞
        import hashlib
        file_ext = ".jpg"  # –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        if 'jpeg' in content_type or 'jpg' in content_type:
            file_ext = ".jpg"
        elif 'png' in content_type:
            file_ext = ".png"
        elif 'gif' in content_type:
            file_ext = ".gif"
        elif 'webp' in content_type:
            file_ext = ".webp"

        filename = f"images/{hashlib.md5(url.encode()).hexdigest()}{file_ext}"

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∞–π–ª
        with open(filename, "wb") as f:
            f.write(response.content)

        print(f"‚úÖ –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {filename}")
        return filename

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è {url}: {e}")
        return ""


# –û–±—Ä–∞–±–æ—Ç–∫–∞ —á–µ—Ä–µ–∑ DeepSeek
def paraphrase_with_deepseek(title: str, text: str) -> str:
    try:
        import requests

        headers = {
            "Authorization": f"Bearer {DEEPSEK_KEY}",
            "Content-Type": "application/json"
        }

        prompt = f"""
–ü–µ—Ä–µ–ø–∏—à–∏ —ç—Ç—É –Ω–æ–≤–æ—Å—Ç—å —Å–≤–æ–∏–º–∏ —Å–ª–æ–≤–∞–º–∏, —Å–æ—Ö—Ä–∞–Ω—è—è –æ—Å–Ω–æ–≤–Ω–æ–π —Å–º—ã—Å–ª –∏ –∫–ª—é—á–µ–≤—ã–µ —Ñ–∞–∫—Ç—ã. –°–¥–µ–ª–∞–π —Ç–µ–∫—Å—Ç –±–æ–ª–µ–µ –∂–∏–≤—ã–º –∏ –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã–º –¥–ª—è —á–∏—Ç–∞—Ç–µ–ª—è.

–û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫: {title}

–¢–µ–∫—Å—Ç –Ω–æ–≤–æ—Å—Ç–∏:
{text}

–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É:
1. –ù–∞–ø–∏—à–∏ –Ω–æ–≤—ã–π –ø—Ä–∏–≤–ª–µ–∫–∞—Ç–µ–ª—å–Ω—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫
2. –ü–µ—Ä–µ—Å–∫–∞–∂–∏ —Ç–µ–∫—Å—Ç —Å–≤–æ–∏–º–∏ —Å–ª–æ–≤–∞–º–∏, —Å–æ—Ö—Ä–∞–Ω—è—è –≤—Å–µ –≤–∞–∂–Ω—ã–µ –¥–µ—Ç–∞–ª–∏
3. –°–¥–µ–ª–∞–π —Ç–µ–∫—Å—Ç –±–æ–ª–µ–µ —á–∏—Ç–∞–±–µ–ª—å–Ω—ã–º –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º
4. –°–æ—Ö—Ä–∞–Ω–∏ —Ç–æ–Ω –Ω–æ–≤–æ—Å—Ç–Ω–æ–π —Å—Ç–∞—Ç—å–∏
5. –ù–µ –¥–æ–±–∞–≤–ª—è–π –ª–∏—à–Ω–µ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, –∫–æ—Ç–æ—Ä–æ–π –Ω–µ—Ç –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª–µ

–í–µ—Ä–Ω–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ:
–ó–ê–ì–û–õ–û–í–û–ö

–¢–µ–∫—Å—Ç –Ω–æ–≤–æ—Å—Ç–∏...
        """

        data = {
            "model": "deepseek-chat",
            "messages": [
                {"role": "user", "content": prompt}
            ],
            "temperature": 0.7,
            "max_tokens": 2000
        }

        response = requests.post("https://api.deepseek.com/v1/chat/completions", headers=headers, json=data, timeout=30)
        response.raise_for_status()

        result = response.json()
        processed_text = result["choices"][0]["message"]["content"].strip()

        print("‚úÖ –¢–µ–∫—Å—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω —á–µ—Ä–µ–∑ DeepSeek")
        return processed_text

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ DeepSeek: {e}")
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
        return f"{title}\n\n{text}"


# –ü–∞—Ä—Å–∏–Ω–≥ RSS –∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤ –æ—á–µ—Ä–µ–¥—å
async def parse_feed_and_process(url: str, limit: int = 5) -> int:
    try:
        print(f"üì° –ü–∞—Ä—Å–∏–º RSS: {url}")

        feed = feedparser.parse(url)
        if feed.entries is None or len(feed.entries) == 0:
            print(f"‚ùå –í RSS-–ª–µ–Ω—Ç–µ –Ω–µ—Ç –∑–∞–ø–∏—Å–µ–π: {url}")
            return 0

        added_count = 0
        for entry in feed.entries[:limit]:
            try:
                link = entry.get("link", "").strip()
                if not link:
                    continue

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –æ—Ç–ø—Ä–∞–≤–ª—è–ª–∏ –ª–∏ —É–∂–µ —ç—Ç—É –Ω–æ–≤–æ—Å—Ç—å
                if await is_news_sent(link) or await is_news_published(link):
                    continue

                # –ü–æ–ª—É—á–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
                title = html.unescape(entry.get("title", "–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞")).strip()

                # –ü–æ–ª—É—á–∞–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ
                summary = html.unescape(entry.get("summary", "")).strip()

                # –ï—Å–ª–∏ –æ–ø–∏—Å–∞–Ω–∏—è –Ω–µ—Ç –∏–ª–∏ –æ–Ω–æ –∫–æ—Ä–æ—Ç–∫–æ–µ, –ø–∞—Ä—Å–∏–º –ø–æ–ª–Ω—É—é —Å—Ç–∞—Ç—å—é
                if not summary or len(summary) < 200:
                    full_text = get_full_article(link)
                    if full_text:
                        summary = full_text
                    elif hasattr(entry, 'content') and entry.content:
                        # –ü—Ä–æ–±—É–µ–º –ø–æ–ª—É—á–∏—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç –∏–∑ –ø–æ–ª—è content
                        content_text = ""
                        for content in entry.content:
                            if hasattr(content, 'value'):
                                content_text += html.unescape(content.value) + "\n"
                        summary = content_text.strip()

                # –ï—Å–ª–∏ –≤—Å–µ –µ—â–µ –Ω–µ—Ç —Ç–µ–∫—Å—Ç–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
                if not summary:
                    summary = title

                # –ü–æ–ª—É—á–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                image_url = ""

                # –ò—â–µ–º –≤ –º–µ–¥–∏–∞-–∫–æ–Ω—Ç–µ–Ω—Ç–µ
                if hasattr(entry, 'media_content') and entry.media_content:
                    for media in entry.media_content:
                        if media.get('type', '').startswith('image/'):
                            image_url = media.get('url', '')
                            if image_url:
                                break

                # –ò—â–µ–º –≤ enclosure
                if not image_url and hasattr(entry, 'enclosures') and entry.enclosures:
                    for enc in entry.enclosures:
                        if enc.get('type', '').startswith('image/'):
                            image_url = enc.get('href', '')
                            if image_url:
                                break

                # –ò—â–µ–º –≤ –æ–ø–∏—Å–∞–Ω–∏–∏
                if not image_url and summary:
                    soup = BeautifulSoup(summary, "html.parser")
                    img_tag = soup.find("img")
                    if img_tag and img_tag.get("src"):
                        image_url = img_tag["src"]

                # –°–∫–∞—á–∏–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                image_path = ""
                if image_url:
                    image_path = download_image(image_url)

                # –î–æ–±–∞–≤–ª—è–µ–º –≤ –æ—á–µ—Ä–µ–¥—å –æ–±—Ä–∞–±–æ—Ç–∫–∏
                await add_to_queue(
                    link=link,
                    title=title,
                    news_text=summary,
                    image_path=image_path,
                    original_title=title,
                    original_text=summary
                )

                # –û—Ç–º–µ—á–∞–µ–º –∫–∞–∫ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–Ω—É—é –Ω–∞ –º–æ–¥–µ—Ä–∞—Ü–∏—é
                await mark_news_sent(link)

                print(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–æ –≤ –æ—á–µ—Ä–µ–¥—å: {title[:50]}...")
                added_count += 1

            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø–∏—Å–∏: {e}")
                continue

        print(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–æ {added_count} –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ {url}")
        return added_count

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ RSS {url}: {e}")
        return 0


# –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–ª–µ–¥—É—é—â–µ–π –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ –æ—á–µ—Ä–µ–¥–∏
async def process_next_from_queue() -> bool:
    try:
        # –ü–æ–ª—É—á–∞–µ–º —Å–ª–µ–¥—É—é—â—É—é –Ω–æ–≤–æ—Å—Ç—å –∏–∑ –æ—á–µ—Ä–µ–¥–∏
        news = await get_next_from_queue()
        if not news:
            print("‚ÑπÔ∏è –í –æ—á–µ—Ä–µ–¥–∏ –Ω–µ—Ç –Ω–æ–≤–æ—Å—Ç–µ–π –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
            return False

        news_id, link, title, news_text, image_path, original_title, original_text, needs_deepseek, deepseek_processed = news

        print(f"üì® –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –Ω–æ–≤–æ—Å—Ç—å –∏–∑ –æ—á–µ—Ä–µ–¥–∏: {title[:50]}...")

        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—É—é –Ω–æ–≤–æ—Å—Ç—å –Ω–∞ –ø–µ—Ä–≤–∏—á–Ω—É—é –º–æ–¥–µ—Ä–∞—Ü–∏—é
        await send_original_news_to_admin(
            original_title=original_title,
            original_text=original_text,
            source_url=link,
            image_path=image_path
        )

        print(f"‚úÖ –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–∞—è –Ω–æ–≤–æ—Å—Ç—å –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–∞ –Ω–∞ –ø–µ—Ä–≤–∏—á–Ω—É—é –º–æ–¥–µ—Ä–∞—Ü–∏—é: {link}")
        return True

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –≤ process_next_from_queue: {e}")
        return False


# –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—á–µ—Ä–µ–¥–∏
async def process_queue_automatically():
    try:
        # –û—á–∏—â–∞–µ–º –∑–∞–≤–∏—Å—à–∏–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏
        await clear_stuck_processing()

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä –æ—á–µ—Ä–µ–¥–∏
        queue_size = await get_queue_size()
        if queue_size == 0:
            return

        print(f"üîÑ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –æ—á–µ—Ä–µ–¥—å ({queue_size} –Ω–æ–≤–æ—Å—Ç–µ–π)")

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–æ 3 –Ω–æ–≤–æ—Å—Ç–µ–π –∑–∞ —Ä–∞–∑
        for _ in range(min(3, queue_size)):
            success = await process_next_from_queue()
            if not success:
                break
            # –ñ–¥–µ–º –º–µ–∂–¥—É –æ–±—Ä–∞–±–æ—Ç–∫–∞–º–∏
            await asyncio.sleep(2)

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –≤ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–µ –æ—á–µ—Ä–µ–¥–∏: {e}")


# –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ RSS
async def periodic_rss_check():
    try:
        sites = await get_sites()
        if not sites:
            print("‚ÑπÔ∏è –ù–µ—Ç RSS-–ª–µ–Ω—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏")
            return

        print(f"üîÑ –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ {len(sites)} RSS-–ª–µ–Ω—Ç...")

        total_added = 0
        for url in sites:
            try:
                added = await parse_feed_and_process(url, limit=2)  # –ë–µ—Ä–µ–º –ø–æ 2 –Ω–æ–≤–æ—Å—Ç–∏ —Å –∫–∞–∂–¥–æ–π –ª–µ–Ω—Ç—ã
                total_added += added
                # –ñ–¥–µ–º –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ –∫ —Ä–∞–∑–Ω—ã–º —Å–∞–π—Ç–∞–º
                await asyncio.sleep(3)
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ {url}: {e}")
                continue

        if total_added > 0:
            print(f"üéØ –í—Å–µ–≥–æ –¥–æ–±–∞–≤–ª–µ–Ω–æ –≤ –æ—á–µ—Ä–µ–¥—å: {total_added} –Ω–æ–≤–æ—Å—Ç–µ–π")

            # –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É –æ—á–µ—Ä–µ–¥–∏ –µ—Å–ª–∏ –µ—Å—Ç—å –Ω–æ–≤—ã–µ –Ω–æ–≤–æ—Å—Ç–∏
            await process_queue_automatically()

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –≤ periodic_rss_check: {e}")
