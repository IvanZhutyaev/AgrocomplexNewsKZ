import requests
from bs4 import BeautifulSoup
from config import SITE_URL, SITE_LOGIN, SITE_PASSWORD
import re
session = requests.Session()


def login_to_site() -> bool:
    """–ê–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –≤ –∞–¥–º–∏–Ω-–ø–∞–Ω–µ–ª–∏ Voyager"""
    login_url = f"{SITE_URL}/admin/login"
    try:
        print("üîë –í—Ö–æ–¥–∏–º –≤ –∞–¥–º–∏–Ω-–ø–∞–Ω–µ–ª—å...")

        # –ü–æ–ª—É—á–∞–µ–º CSRF —Ç–æ–∫–µ–Ω —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ª–æ–≥–∏–Ω–∞
        login_page = session.get(login_url, timeout=10)
        soup = BeautifulSoup(login_page.text, "html.parser")
        token_tag = soup.find("input", {"name": "_token"})
        csrf_token = token_tag["value"] if token_tag else None

        if not csrf_token:
            print("‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω CSRF —Ç–æ–∫–µ–Ω –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ –ª–æ–≥–∏–Ω–∞")
            return False

        data = {
            "email": SITE_LOGIN,
            "password": SITE_PASSWORD,
            "_token": csrf_token
        }

        resp = session.post(login_url, data=data, timeout=10)
        if "voyager-dashboard" in resp.text or "logout" in resp.text:
            print("‚úÖ –ê–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –ø—Ä–æ—à–ª–∞ —É—Å–ø–µ—à–Ω–æ")
            return True
        else:
            print("‚ö†Ô∏è –ê–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –Ω–µ —É–¥–∞–ª–∞—Å—å. –ü—Ä–æ–≤–µ—Ä—å –ª–æ–≥–∏–Ω/–ø–∞—Ä–æ–ª—å.")
            return False
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏: {e}")
        return False


def extract_title_and_body(text: str):
    """–†–∞–∑–¥–µ–ª—è–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∏ —Ç–µ–ª–æ"""
    # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
    text = text.strip()

    # –ò—â–µ–º —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å - –¥–≤–æ–π–Ω–æ–π –ø–µ—Ä–µ–Ω–æ—Å —Å—Ç—Ä–æ–∫–∏
    if "\n\n" in text:
        parts = text.split("\n\n", 1)
        title = parts[0].strip()
        body = parts[1].strip()
    else:
        # –ò—â–µ–º –ø–µ—Ä–≤—ã–π –æ–¥–∏–Ω–∞—Ä–Ω—ã–π –ø–µ—Ä–µ–Ω–æ—Å —Å—Ç—Ä–æ–∫–∏
        lines = text.split("\n")
        if len(lines) > 1:
            title = lines[0].strip()
            body = "\n".join(lines[1:]).strip()
        else:
            # –¢–æ–ª—å–∫–æ –æ–¥–Ω–∞ —Å—Ç—Ä–æ–∫–∞
            title = text
            body = ""

    # –û—á–∏—â–∞–µ–º –∏ –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
    title = re.sub(r'\s+', ' ', title)  # –ó–∞–º–µ–Ω—è–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã –Ω–∞ –æ–¥–∏–Ω
    title = title[:255]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É

    print(f"üìÑ –ò–∑–≤–ª–µ—á–µ–Ω –∑–∞–≥–æ–ª–æ–≤–æ–∫: {title}")
    print(f"üìÑ –ò–∑–≤–ª–µ—á–µ–Ω —Ç–µ–∫—Å—Ç: {len(body)} —Å–∏–º–≤–æ–ª–æ–≤")

    return title, body


def get_csrf_token_for_create() -> str:
    """–ü–æ–ª—É—á–∞–µ–º CSRF —Ç–æ–∫–µ–Ω —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å–æ–∑–¥–∞–Ω–∏—è –Ω–æ–≤–æ—Å—Ç–∏"""
    create_url = f"{SITE_URL}/admin/news/create"
    try:
        resp = session.get(create_url, timeout=10)
        if resp.status_code != 200:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å–æ–∑–¥–∞–Ω–∏—è: {resp.status_code}")
            return None
        soup = BeautifulSoup(resp.text, "html.parser")
        token_tag = soup.find("meta", {"name": "csrf-token"})
        token = token_tag["content"] if token_tag else None
        if not token:
            print("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ CSRF —Ç–æ–∫–µ–Ω –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ —Å–æ–∑–¥–∞–Ω–∏—è")
        return token
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è CSRF —Ç–æ–∫–µ–Ω–∞: {e}")
        return None


def post_news_to_site(news_text: str, image_path: str = None) -> bool:
    """–°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ —á–µ—Ä–µ–∑ Voyager admin"""
    if not login_to_site():
        return False

    csrf_token = get_csrf_token_for_create()
    if not csrf_token:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å CSRF —Ç–æ–∫–µ–Ω –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –Ω–æ–≤–æ—Å—Ç–∏")
        return False

    create_url = f"{SITE_URL}/admin/news"
    title, body = extract_title_and_body(news_text)

    # –õ–æ–≥–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
    print(f"üìù –î–∞–Ω–Ω—ã–µ –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏:")
    print(f"–ó–∞–≥–æ–ª–æ–≤–æ–∫ ({len(title)} —Å–∏–º–≤.): {title}")
    print(f"–¢–µ–∫—Å—Ç ({len(body)} —Å–∏–º–≤.): {body[:100]}..." if body else "–¢–µ–∫—Å—Ç –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç")
    print(f"CSRF —Ç–æ–∫–µ–Ω: {csrf_token[:20]}...")

    # –ü—Ä–∞–≤–∏–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞ —Ñ–æ—Ä–º—ã
    data = {
        "_token": csrf_token,
        # –û—Å–Ω–æ–≤–Ω—ã–µ –ø–æ–ª—è
        "title": title,
        "subtitle": title[:100],  # –∫—Ä–∞—Ç–∫–∏–π –∑–∞–≥–æ–ª–æ–≤–æ–∫
        "description": body,
        # SEO –ø–æ–ª—è
        "seo_title": title[:60],
        "seo_description": body[:160] if body else title[:160],
        "seo_keywords": ", ".join(title.split()[:5]),
        "seo_slug": "",
        # –°–∫—Ä—ã—Ç—ã–µ –ø–æ–ª—è i18n (–º—É–ª—å—Ç–∏—è–∑—ã—á–Ω–æ—Å—Ç—å)
        "title_i18n": '{"ru":"' + title + '","kk":"' + title + '","en":"' + title + '","zh":"' + title + '"}',
        "subtitle_i18n": '{"ru":"' + title[:100] + '","kk":"' + title[:100] + '","en":"' + title[
                                                                                           :100] + '","zh":"' + title[
                                                                                                                :100] + '"}',
        "description_i18n": '{"ru":"' + body + '","kk":"' + body + '","en":"' + body + '","zh":"' + body + '"}',
        "seo_title_i18n": '{"ru":"' + title[:60] + '","kk":"' + title[:60] + '","en":"' + title[
                                                                                          :60] + '","zh":"' + title[
                                                                                                              :60] + '"}',
        "seo_description_i18n": '{"ru":"' + (body[:160] if body else title[:160]) + '","kk":"' + (
            body[:160] if body else title[:160]) + '","en":"' + (body[:160] if body else title[:160]) + '","zh":"' + (
                                    body[:160] if body else title[:160]) + '"}',
        "seo_keywords_i18n": '{"ru":"' + ", ".join(title.split()[:5]) + '","kk":"' + ", ".join(
            title.split()[:5]) + '","en":"' + ", ".join(title.split()[:5]) + '","zh":"' + ", ".join(
            title.split()[:5]) + '"}',
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è
        "redirect_to": "",
        "model_name": "App\\Models\\News",
        "model_id": "",
        "type_slug": "news",
    }

    files = {}
    if image_path:
        try:
            files["image"] = open(image_path, "rb")
            print(f"üñºÔ∏è –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {image_path}")
        except Exception as e:
            print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–∫—Ä—ã—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {e}")

    try:
        print(f"üåê –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –Ω–∞: {create_url}")
        response = session.post(create_url, data=data, files=files, timeout=20)

        if files:
            files["image"].close()

        print(f"üì° –û—Ç–≤–µ—Ç —Å–µ—Ä–≤–µ—Ä–∞: {response.status_code}")

        # –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –æ—Ç–≤–µ—Ç–∞
        if response.status_code == 500:
            print("‚ùå –û—à–∏–±–∫–∞ 500 - –≤–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –æ—à–∏–±–∫–∞ —Å–µ—Ä–≤–µ—Ä–∞")
            print("üîç –¢–µ–ª–æ –æ—Ç–≤–µ—Ç–∞:")
            error_text = response.text
            print(error_text[:1500])  # –ë–æ–ª—å—à–µ —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞

            # –ò—â–µ–º –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É—é –æ—à–∏–±–∫—É –≤ —Ç–µ–∫—Å—Ç–µ
            if "title_i18n" in error_text:
                print("‚ö†Ô∏è –ü—Ä–æ–±–ª–µ–º–∞ —Å –ø–æ–ª–µ–º title_i18n")
            if "description_i18n" in error_text:
                print("‚ö†Ô∏è –ü—Ä–æ–±–ª–µ–º–∞ —Å –ø–æ–ª–µ–º description_i18n")

            return False

        if response.status_code in (200, 302):
            if "voyager/news" in response.text or response.status_code == 302:
                print("üåê –ù–æ–≤–æ—Å—Ç—å —É—Å–ø–µ—à–Ω–æ –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–∞ –Ω–∞ —Å–∞–π—Ç–µ!")
                return True
            else:
                print("‚ö†Ô∏è –û—Ç–≤–µ—Ç –Ω–µ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞–µ—Ç —É—Å–ø–µ—à–Ω–æ–µ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ.")
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –Ω–∞–ª–∏—á–∏–µ —Å–æ–æ–±—â–µ–Ω–∏–π –æ–± —É—Å–ø–µ—Ö–µ
                success_keywords = ['—É—Å–ø–µ—Ö', '—Å–æ–∑–¥–∞–Ω', '–¥–æ–±–∞–≤–ª–µ–Ω', 'success', 'created', 'added']
                if any(keyword in response.text.lower() for keyword in success_keywords):
                    print("‚úÖ –ü–æ—Ö–æ–∂–µ, –Ω–æ–≤–æ—Å—Ç—å –¥–æ–±–∞–≤–ª–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ (–Ω–∞–π–¥–µ–Ω—ã –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞)")
                    return True

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –æ—à–∏–±–æ–∫ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
                error_keywords = ['error', '–æ—à–∏–±–∫–∞', 'validation', '–≤–∞–ª–∏–¥–∞—Ü–∏—è']
                if any(keyword in response.text.lower() for keyword in error_keywords):
                    print("‚ùå –ù–∞–π–¥–µ–Ω—ã –æ—à–∏–±–∫–∏ –≤ –æ—Ç–≤–µ—Ç–µ")
                    # –ò—â–µ–º –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –æ—à–∏–±–∫–∏
                    soup = BeautifulSoup(response.text, "html.parser")
                    errors = soup.find_all(class_=['error', 'alert-danger', 'validation-error'])
                    for error in errors:
                        print(f"–û—à–∏–±–∫–∞: {error.get_text(strip=True)}")

                return False
        else:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏: {response.status_code}")
            print(f"–¢–µ–∫—Å—Ç –æ—Ç–≤–µ—Ç–∞: {response.text[:500]}")
            return False

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ç–ø—Ä–∞–≤–∫–µ –Ω–æ–≤–æ—Å—Ç–∏: {e}")
        return False


def analyze_create_form():
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ñ–æ—Ä–º—É —Å–æ–∑–¥–∞–Ω–∏—è –Ω–æ–≤–æ—Å—Ç–∏ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏"""
    if not login_to_site():
        return

    create_url = f"{SITE_URL}/admin/news/create"
    try:
        resp = session.get(create_url, timeout=10)
        soup = BeautifulSoup(resp.text, "html.parser")

        print("üîç –ê–Ω–∞–ª–∏–∑ —Ñ–æ—Ä–º—ã —Å–æ–∑–¥–∞–Ω–∏—è –Ω–æ–≤–æ—Å—Ç–∏:")

        # –ò—â–µ–º –≤—Å–µ –ø–æ–ª—è —Ñ–æ—Ä–º—ã
        form = soup.find('form')
        if form:
            inputs = form.find_all('input')
            textareas = form.find_all('textarea')
            selects = form.find_all('select')

            print(f"üìã –ù–∞–π–¥–µ–Ω–æ –ø–æ–ª–µ–π:")
            print(f"Inputs: {len(inputs)}")
            for inp in inputs:
                name = inp.get('name', '–±–µ–∑ –∏–º–µ–Ω–∏')
                type_ = inp.get('type', '–±–µ–∑ —Ç–∏–ø–∞')
                print(f"  - {name} (type: {type_})")

            print(f"Textareas: {len(textareas)}")
            for ta in textareas:
                name = ta.get('name', '–±–µ–∑ –∏–º–µ–Ω–∏')
                print(f"  - {name}")

            print(f"Selects: {len(selects)}")
            for sel in selects:
                name = sel.get('name', '–±–µ–∑ –∏–º–µ–Ω–∏')
                print(f"  - {name}")

        # –ò—â–µ–º CSRF —Ç–æ–∫–µ–Ω
        token_tag = soup.find("meta", {"name": "csrf-token"})
        if token_tag:
            print(f"‚úÖ CSRF —Ç–æ–∫–µ–Ω –Ω–∞–π–¥–µ–Ω: {token_tag['content'][:20]}...")
        else:
            print("‚ùå CSRF —Ç–æ–∫–µ–Ω –Ω–µ –Ω–∞–π–¥–µ–Ω")

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Ñ–æ—Ä–º—ã: {e}")


def check_required_fields():
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è —Ñ–æ—Ä–º—ã"""
    if not login_to_site():
        return

    create_url = f"{SITE_URL}/admin/news/create"
    try:
        resp = session.get(create_url, timeout=10)
        soup = BeautifulSoup(resp.text, "html.parser")

        print("üîç –ü–æ–∏—Å–∫ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –ø–æ–ª–µ–π:")

        # –ò—â–µ–º –ø–æ–ª—è —Å –∞—Ç—Ä–∏–±—É—Ç–æ–º required
        required_fields = []
        all_inputs = soup.find_all('input')
        all_textareas = soup.find_all('textarea')
        all_selects = soup.find_all('select')

        for field in all_inputs + all_textareas + all_selects:
            if field.get('required'):
                name = field.get('name', '–±–µ–∑ –∏–º–µ–Ω–∏')
                required_fields.append(name)
                field_type = field.name
                print(f"‚ö†Ô∏è –û–±—è–∑–∞—Ç–µ–ª—å–Ω–æ–µ –ø–æ–ª–µ: {name} (—Ç–∏–ø: {field_type})")

        if not required_fields:
            print("‚úÖ –û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –ø–æ–ª–µ–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")

        return required_fields

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø–æ–ª–µ–π: {e}")
        return []


def analyze_real_form_fields():
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ä–µ–∞–ª—å–Ω—ã–µ –ø–æ–ª—è —Ñ–æ—Ä–º—ã —á–µ—Ä–µ–∑ JavaScript –∏–ª–∏ HTML"""
    if not login_to_site():
        return

    create_url = f"{SITE_URL}/admin/news/create"
    try:
        resp = session.get(create_url, timeout=10)

        print("üîç –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ñ–æ—Ä–º—ã:")
        print("=" * 50)

        # –ò—â–µ–º –≤—Å–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ –ø–æ–ª—è –≤–≤–æ–¥–∞
        soup = BeautifulSoup(resp.text, "html.parser")

        # –í—Å–µ input —ç–ª–µ–º–µ–Ω—Ç—ã
        print("üìã INPUT –ø–æ–ª—è:")
        inputs = soup.find_all('input')
        for inp in inputs:
            name = inp.get('name', '–±–µ–∑ –∏–º–µ–Ω–∏')
            input_type = inp.get('type', 'text')
            value = inp.get('value', '')
            placeholder = inp.get('placeholder', '')
            print(f"  - name: '{name}', type: '{input_type}', value: '{value}', placeholder: '{placeholder}'")

        # –í—Å–µ textarea —ç–ª–µ–º–µ–Ω—Ç—ã
        print("\nüìã TEXTAREA –ø–æ–ª—è:")
        textareas = soup.find_all('textarea')
        for ta in textareas:
            name = ta.get('name', '–±–µ–∑ –∏–º–µ–Ω–∏')
            placeholder = ta.get('placeholder', '')
            print(f"  - name: '{name}', placeholder: '{placeholder}'")

        # –í—Å–µ select —ç–ª–µ–º–µ–Ω—Ç—ã
        print("\nüìã SELECT –ø–æ–ª—è:")
        selects = soup.find_all('select')
        for sel in selects:
            name = sel.get('name', '–±–µ–∑ –∏–º–µ–Ω–∏')
            options = sel.find_all('option')
            print(f"  - name: '{name}', options: {len(options)}")
            for opt in options[:3]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 3 –æ–ø—Ü–∏–∏
                print(f"    * {opt.get('value', '')} - {opt.text}")

        # –ò—â–µ–º JavaScript –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏
        print("\nüîç JavaScript –¥–∞–Ω–Ω—ã–µ:")
        scripts = soup.find_all('script')
        for script in scripts:
            if script.string:
                js_content = script.string
                # –ò—â–µ–º —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –ø–æ–ª–µ–π –≤ JS
                if any(field in js_content for field in ['title', 'body', 'content', 'description']):
                    lines = js_content.split('\n')
                    for line in lines[:10]:  # –ü–µ—Ä–≤—ã–µ 10 —Å—Ç—Ä–æ–∫
                        if any(field in line for field in ['title', 'body', 'content', 'description']):
                            print(f"  JS: {line.strip()}")

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Ñ–æ—Ä–º—ã: {e}")